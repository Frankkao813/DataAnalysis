{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MM_HW02 B07102052 高維謙\n",
    "\n",
    "background: It is the assignment of management mathematics course offered by department of information management. The course assignment material is usually not directly related to the course, and students are required to do self-learning to finish the assignment. \n",
    "\n",
    "The instructions for this assignment is in the link below:\n",
    "shorturl.at/boDI1 (`MM_Assignmnet_02_2022.pdf`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "Prepare the required package and the needed parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "N = 2000\n",
    "mu = 3.5\n",
    "sd = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1-a\n",
    "In this question, we are asked to generate four datasets: \n",
    "1. `high school math GPA`: 2.0 - 4.0, continuous\n",
    "2. `(binary variable)high school calculus status`: 1 -yes, 0-no\n",
    "3. `(binary variable) ntu precalculus status`: 1-yes, 0-no\n",
    "4. `ntu calculus 1 gpa`: 0.0, 0.7 - 4.0 in 0.1 increments\n",
    "\n",
    "In 2, 3, binomial distributions are used, and different probabilities will be applied in 2,3. For instance, in 2, we have to contain the following information. In order to obtain probability, I will first construct a probability array `prob_precalc`, then stuff it into `np.random.binomial()`\n",
    "\n",
    "```\n",
    "# simulate previous calculus in high school:\n",
    "# - 75% of people with HS math GPAs over 3.6 took HS calculus\n",
    "# - 40% of people with HS math GPAs under 3.6 took HS calculus\n",
    "# binomial data with 1 trial per student, probabilities as above\n",
    "\n",
    "```\n",
    "\n",
    "In 4, generate the dataset by using the below equation, then round to the nearest 0.1 to satisfy the requirement of the question.\n",
    "\\begin{equation}\n",
    "\\text{ntu Calculus I grade}_{i} = 0.3 + 0.7 * \\text{HS math GPA}_i + 0.3* \\text{HS calculus}_i + 0.1 * \\text{ntu precalculus}_i + \\epsilon_i \\sim Normal(mean = 0, SD = 0.5)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing hs_math_gpa data\n",
    "hs_math_gpa = np.random.normal(mu, sd, N)\n",
    "# truncate\n",
    "for idx, ele in enumerate(hs_math_gpa):\n",
    "    if(hs_math_gpa[idx] < 2.0):\n",
    "        hs_math_gpa[idx] = 2.0\n",
    "    elif(hs_math_gpa[idx] > 4.0):\n",
    "        hs_math_gpa[idx] = 4.0\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "\n",
    "# prepare hs_calculus data\n",
    "prob_hs_calc = []\n",
    "for i in range(len(hs_math_gpa)):\n",
    "    if(hs_math_gpa[i] >= 3.6):\n",
    "        prob_hs_calc.append(0.75)\n",
    "    else:\n",
    "        prob_hs_calc.append(0.40)\n",
    "\n",
    "hs_calculus = np.random.binomial(1, prob_hs_calc, N) \n",
    "\n",
    "# prepare ntu_precalc data\n",
    "prob_precalc = []\n",
    "for i in range(len(hs_math_gpa)):\n",
    "    if(hs_calculus[i] == 0): # no high school calculus\n",
    "        prob_precalc.append(0.7)\n",
    "    else: # with high school calculus\n",
    "        if(hs_math_gpa[i] < 3.5):\n",
    "            prob_precalc.append(0.60)\n",
    "        else:\n",
    "            prob_precalc.append(0.25)\n",
    "ntu_precalc = np.random.binomial(1, prob_precalc, N) \n",
    "#print(ntu_precalc)\n",
    "\n",
    "# prepare NTU calculus 1 GPA data\n",
    "error = np.random.normal(0, sd, N)\n",
    "true_beta = {\"intercept\": 0.3, \"hs_math_gpa\": 0.7, \n",
    "            \"hs_calculus\": 0.3, \"ntu_precalc\": 0.1}\n",
    "ntu_calculus_gpa = true_beta[\"intercept\"] + true_beta[\"hs_math_gpa\"] * hs_math_gpa + true_beta[\"hs_calculus\"]* hs_calculus + \\\n",
    "                    true_beta[\"ntu_precalc\"]*ntu_precalc + error\n",
    "\n",
    "# round\n",
    "for i in range(len(ntu_calculus_gpa)):\n",
    "    ntu_calculus_gpa[i] = round(ntu_calculus_gpa[i], 1)\n",
    "    if(ntu_calculus_gpa[i] > 4.0):\n",
    "        ntu_calculus_gpa[i] = 4.0\n",
    "    elif(ntu_calculus_gpa[i] < 0.7):\n",
    "        ntu_calculus_gpa[i] = 0.7\n",
    "    else:\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "discussion:\n",
    "\n",
    "We can draw the graphs about the `ntu calculus I gpa distribution` and `error distribution`. We can see that gpa distributions skew to the left, and error distributions appears approximately normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ntu_calculus_gpa, bins = 50)\n",
    "plt.title(\"NTU calculus I gpa distribution\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(error, bins = 100)\n",
    "plt.title(\"NTU calculus I error distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1-b\n",
    "The question requires us to fit the datasets using multiple regression methods. \n",
    "\n",
    "Steps:\n",
    "1. constructing dataframe using `pd.DataFrame()`\n",
    "2. use `sm.OLS.from_formula()`  and `model.params` to obtain the coeffecients.\n",
    "\n",
    "The result is in the second code cell in Q1-b. The coefficient is close to the true value as stated in Q1-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hs_math_gpa</th>\n",
       "      <th>hs_calculus</th>\n",
       "      <th>ntu_precalc</th>\n",
       "      <th>error</th>\n",
       "      <th>ntu_calculus_gpa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.052097</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.305898</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.662202</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.745060</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.001101</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.711094</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.245332</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.733717</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.386334</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hs_math_gpa  hs_calculus  ntu_precalc     error  ntu_calculus_gpa\n",
       "0     4.000000            0            1  1.052097               4.0\n",
       "1     3.305898            1            0 -0.662202               2.3\n",
       "2     3.745060            1            0 -0.001101               3.2\n",
       "3     3.711094            1            0  0.245332               3.4\n",
       "4     3.733717            1            0  0.386334               3.6"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct dataframe\n",
    "# the result seems to be a little bit weird?\n",
    "data = {\n",
    "    \"hs_math_gpa\": hs_math_gpa,\n",
    "    \"hs_calculus\": hs_calculus,\n",
    "    \"ntu_precalc\": ntu_precalc,\n",
    "    \"error\": error,\n",
    "    \"ntu_calculus_gpa\": ntu_calculus_gpa \n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept      0.371928\n",
      "hs_math_gpa    0.672928\n",
      "hs_calculus    0.286466\n",
      "ntu_precalc    0.113415\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "model1 = sm.OLS.from_formula('ntu_calculus_gpa~ hs_math_gpa + hs_calculus + ntu_precalc', data = df).fit()\n",
    "print(model1.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1-c\n",
    "We are asked to set up the matrix X and variable y, where\n",
    "$$\n",
    "X=\n",
    "\\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & x_{13} \\\\\n",
    "1 & x_{21} & x_{22} & x_{23} \\\\\n",
    "1 & x_{31} & x_{32} & x_{33} \\\\\n",
    "...&...    &...     &...     \\\\\n",
    "1 & x_{2000,1} & x_{2000,2} & x_{2000,3} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y=\n",
    "\\begin{bmatrix}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "y_3\\\\\n",
    "...\\\\\n",
    "y_{2000}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Steps:\n",
    "1. prepare the intercept and the independent variable datasets\n",
    "2. construct a matrix using `np.vstack()`, then transpose. The result is stored into A\n",
    "3. stuff ntu calculus gpa dataset into Y variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 4)\n"
     ]
    }
   ],
   "source": [
    "# setting up the matrix X\n",
    "intercept_data = [1]* 2000\n",
    "X = np.vstack((intercept_data, hs_math_gpa, hs_calculus, ntu_precalc)).T\n",
    "print(X.shape)\n",
    "Y = ntu_calculus_gpa # the transpose of the array will not change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 1-d\n",
    "In this question, we have to compute three values: (let C = $ X^T X $). The way to calculate the values are listed below each bullet points.\n",
    "* $C^{-1}$:\n",
    "First calculate C using `np.dot()`, then inverse it using `np.linalg.inv()`\n",
    "* $adj(C)$:\n",
    "This one is a little bit tricky, because there is no function directly corresponding to adjoint matrix.\n",
    "\n",
    "We derive the equation as follows:\n",
    "\\begin{equation}\n",
    "A^{-1} = Adj(A) / det(A)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "Adj(A) = (cofactor(A))^T\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "A^{-1} = (1/det(A)) * (cofactor(A))^T\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "(cofactor(A))^T = det(A) * A^{-1}\n",
    "\\end{equation}\n",
    "\n",
    "* det(C): `np.linalg.det()`\n",
    "\n",
    "\n",
    "discussion:\n",
    "* about the matrix size: size of  $X^T$:(2000 * 4), $X$: (4* 2000), and the size after multiplication is (4*4), which corresponds to the size in the output cell.\n",
    "\n",
    "\n",
    "source: \n",
    "* How to Find cofactor of a matrix using Numpy\n",
    "https://www.geeksforgeeks.org/how-to-find-cofactor-of-a-matrix-using-numpy/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the determinant of C\n",
      "140385679387.9601\n",
      "_____________________________\n",
      "the adjoint of C\n",
      "[[ 5.53009727e+09 -1.50599881e+09  2.54431561e+07 -5.08154126e+08]\n",
      " [-1.50599881e+09  4.34709504e+08 -7.46152977e+07  8.14027140e+07]\n",
      " [ 2.54431561e+07 -7.46152977e+07  3.35495448e+08  9.91708610e+07]\n",
      " [-5.08154126e+08  8.14027140e+07  9.91708610e+07  3.35905975e+08]]\n",
      "______________________________\n",
      "inverse of C\n",
      "[[ 0.03939218 -0.01072758  0.00018124 -0.0036197 ]\n",
      " [-0.01072758  0.00309654 -0.0005315   0.00057985]\n",
      " [ 0.00018124 -0.0005315   0.00238981  0.00070642]\n",
      " [-0.0036197   0.00057985  0.00070642  0.00239274]]\n"
     ]
    }
   ],
   "source": [
    "# compute matrix quantities (X^T X)^(-1)\n",
    "C = np.dot(X.T, X)\n",
    "C_with_inv = np.linalg.inv(C) # with inverse\n",
    "det_C = np.linalg.det(C)\n",
    "adj_C = C_with_inv.T * det_C\n",
    "print(\"the determinant of C\")\n",
    "print(det_C)\n",
    "print(\"_____________________________\")\n",
    "print(\"the adjoint of C\")\n",
    "print(adj_C)\n",
    "print(\"______________________________\")\n",
    "print(\"inverse of C\")\n",
    "print(C_with_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1-e\n",
    "We are aked to compute the following values\n",
    "* beta:\n",
    "\\begin{equation} \n",
    "\\hat\\beta = (X^TX)^{-1}X^Ty\n",
    "\\end{equation} \n",
    "\n",
    "* residuals:\n",
    "\\begin{equation} \n",
    "\\hat\\sigma^2 = \\frac{(y-X\\hat\\beta)^T(y-X\\hat\\beta)}{(n-p-1)}\n",
    "\\end{equation}\n",
    "\n",
    "* standard error of the covariance matrix:\n",
    "\\begin{equation}\n",
    "\\hat{var}(\\hat\\beta) = \\hat\\sigma^2(X^TX)^{-1}\n",
    "\\end{equation} \n",
    "\n",
    "discussion:\n",
    "\n",
    "The diagonal term of the `var_beta` below corresponds to to the variance of each coefficient. \n",
    "$$cov(X, X) = var(x)$$\n",
    "One can square the diagonal terms to get the standard error of each coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the result of beta matrix\n",
      "[0.371928   0.67292813 0.28646624 0.11341463]\n",
      "--------------------------\n",
      "the result of residual noise:\n",
      "0.24062232200910832\n",
      "----------------------------------\n",
      "covariance for linear regression coeffecient estimate\n",
      "[[ 9.47863666e-03 -2.58129556e-03  4.36097992e-05 -8.70980760e-04]\n",
      " [-2.58129556e-03  7.45096015e-04 -1.27891294e-04  1.39524987e-04]\n",
      " [ 4.36097992e-05 -1.27891294e-04  5.75042227e-04  1.69979751e-04]\n",
      " [-8.70980760e-04  1.39524987e-04  1.69979751e-04  5.75745874e-04]]\n",
      "----------------------------------------------\n",
      "standard error for each coefficient\n",
      "[0.09735829 0.02729645 0.02398004 0.02399471]\n"
     ]
    }
   ],
   "source": [
    "# calculate beta matrix\n",
    "mid_prod = np.dot(X.T, Y)\n",
    "beta_matrix = np.dot(C_with_inv, mid_prod)\n",
    "print(\"the result of beta matrix\")\n",
    "print(beta_matrix)\n",
    "print(\"--------------------------\")\n",
    "\n",
    "# y - X * beta_matrix\n",
    "prod1 = (Y - np.dot(X, beta_matrix)).T\n",
    "prod2 = Y - np.dot(X, beta_matrix)\n",
    "residual_noise = np.dot(prod1, prod2)/(N - 3 - 1)\n",
    "print(\"the result of residual noise:\")\n",
    "print(residual_noise) \n",
    "print(\"----------------------------------\")\n",
    "\n",
    "var_beta = residual_noise *C_with_inv\n",
    "print(\"covariance for linear regression coeffecient estimate\")\n",
    "print(var_beta)\n",
    "print(\"----------------------------------------------\")\n",
    "\n",
    "# take the square root of var(beta) to get the standard errors for each coefficient\n",
    "beta_SE = np.diag(var_beta, k = 0) ** 0.5\n",
    "print(\"standard error for each coefficient\")\n",
    "print(beta_SE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1-f\n",
    "The result is already computed in Q1-e. We can make the following table to compare coefficients, which are shown in the following code cell result.\n",
    "\n",
    "From the result, we can find that the the value calculated by manual and lm is the same, and are slightly different from the true value because of the error term added into the function ($\\epsilon_i \\sim Normal(0, 0.5)$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Truth</th>\n",
       "      <th>Manual</th>\n",
       "      <th>lm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.371928</td>\n",
       "      <td>0.371928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hs_math_gpa</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.672928</td>\n",
       "      <td>0.672928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hs_calculus</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.286466</td>\n",
       "      <td>0.286466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ntu_precalc</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.113415</td>\n",
       "      <td>0.113415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Truth    Manual        lm\n",
       "intercept      0.3  0.371928  0.371928\n",
       "hs_math_gpa    0.7  0.672928  0.672928\n",
       "hs_calculus    0.3  0.286466  0.286466\n",
       "ntu_precalc    0.1  0.113415  0.113415"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Truth = [0.3, 0.7, 0.3, 0.1]\n",
    "Manual = beta_matrix\n",
    "lm = [i for i in model1.params]\n",
    "data = {\"Truth\": Truth, \"Manual\": Manual, \"lm\": lm}\n",
    "df_comp = pd.DataFrame(data, index = [\"intercept\", \"hs_math_gpa\", \"hs_calculus\", \"ntu_precalc\"])\n",
    "df_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1-g\n",
    "\n",
    "We are asked to calculate the estimated standard error of parameter estimates. There will be three groups of result: Truth, Manual, and lm. \n",
    "* Truth: The true covariance is obtained by altering  $\\hat \\sigma ^2 $ in formula (3) into $\\sigma ^ 2$, and the result is obtained by extracting diagonal elements then take square root.\n",
    "* Manual: Using the property that $\\hat \\sigma ^2 $ is the unbiased estimator of $\\sigma ^ 2$, we use the `beta_SE` obtained in Q1-e\n",
    "* lm: the value can be obtained by using `model.bse`\n",
    "\n",
    "In Q1-h, we will arrange the data into pandas dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true standard error\n",
      "[0.09923731 0.02782327 0.02444285 0.0244578 ]\n",
      "-------------------------------------------\n",
      "standard error of beta by manual calculation\n",
      "[0.09735829 0.02729645 0.02398004 0.02399471]\n",
      "--------------------------------------------\n",
      "standard error of beta by linear model in python\n",
      "[0.09735829012102729, 0.027296446926939496, 0.023980038084450057, 0.02399470513339941]\n"
     ]
    }
   ],
   "source": [
    "true_cov = pow(sd,2) * C_with_inv\n",
    "true_SE = np.sqrt(np.diag(true_cov))\n",
    "print(\"true standard error\")\n",
    "print(true_SE)\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "print(\"standard error of beta by manual calculation\")\n",
    "print(beta_SE)\n",
    "print(\"--------------------------------------------\")\n",
    "# https://stackoverflow.com/questions/31523921/print-std-err-value-from-statsmodels-ols-results\n",
    "# from statsmodels\n",
    "#print(model1.summary()) # how to extract stndard error?\n",
    "print(\"standard error of beta by linear model in python\")\n",
    "model_bse = [i for i in model1.bse]\n",
    "print(model_bse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After arranging the data, we can obtain the following table. We can observe that the result of manual calculation is the same as lm function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Truth</th>\n",
       "      <th>Manual</th>\n",
       "      <th>lm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>0.099237</td>\n",
       "      <td>0.097358</td>\n",
       "      <td>0.097358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hs_math_gpa</th>\n",
       "      <td>0.027823</td>\n",
       "      <td>0.027296</td>\n",
       "      <td>0.027296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hs_calculus</th>\n",
       "      <td>0.024443</td>\n",
       "      <td>0.023980</td>\n",
       "      <td>0.023980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ntu_precalc</th>\n",
       "      <td>0.024458</td>\n",
       "      <td>0.023995</td>\n",
       "      <td>0.023995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Truth    Manual        lm\n",
       "intercept    0.099237  0.097358  0.097358\n",
       "hs_math_gpa  0.027823  0.027296  0.027296\n",
       "hs_calculus  0.024443  0.023980  0.023980\n",
       "ntu_precalc  0.024458  0.023995  0.023995"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = {\"Truth\": true_SE, \"Manual\": beta_SE, \"lm\": model_bse}\n",
    "df_error = pd.DataFrame(error, index = [\"intercept\", \"hs_math_gpa\", \"hs_calculus\", \"ntu_precalc\"])\n",
    "df_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1-h\n",
    "\n",
    "We are asked to compare the estimated residual variance. The way to calculate the values are listed below:\n",
    "* True value: 0.5(the standard deviation specified in the question) **  2\n",
    "* Manual calulation: The formula stated in Q1-e\n",
    "* lm: `model.mde_resid`\n",
    "\n",
    "From the table below, we can see that result of Manual calculation and lm is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Truth</th>\n",
       "      <th>Manual</th>\n",
       "      <th>lm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>residual var</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.240622</td>\n",
       "      <td>0.240622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Truth    Manual        lm\n",
       "residual var   0.25  0.240622  0.240622"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_residual_var = sd ** 2\n",
    "residual_var = residual_noise\n",
    "lm_residual_var = model1.mse_resid\n",
    "residual_comp = {\"Truth\": true_residual_var, \"Manual\": residual_var, \"lm\": residual_var}\n",
    "residual_comp_df = pd.DataFrame(residual_comp, index = [\"residual var\"])\n",
    "residual_comp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1-i\n",
    "using the coefficient obtained in Q1-f, we compute the value using the following formula:\n",
    "$$coeff[0] + coeff[1]* HS\\_math\\_GPA + coeff[2]*HS\\_calc+coeff[3]*NTU\\_precalc$$\n",
    "where `HS_math_GPA`, `HS_calc`, and `NTU_precalc` value are stated in the question. and `coeff` can be replaced as `Truth`, `Manual`, or `lm`\n",
    "\n",
    "After calculation and some post-processing, one can obtained the table below. From the table, we can see that the value predicted by Manual and lm is the same, and the True value is slightlt higher than the manual and lm value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Truth</th>\n",
       "      <th>Manual</th>\n",
       "      <th>lm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction</th>\n",
       "      <td>2.786419</td>\n",
       "      <td>2.760256</td>\n",
       "      <td>2.760256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Truth    Manual        lm\n",
       "prediction  2.786419  2.760256  2.760256"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the coeffiecient in Q1-f\n",
    "# Truth\n",
    "HS_math_GPA = 3.123456\n",
    "HS_calc = 1\n",
    "NTU_precalc = 0\n",
    "\n",
    "pred_truth = Truth[0] + Truth[1]* HS_math_GPA + Truth[2] * HS_calc + Truth[3] * NTU_precalc\n",
    "pred_manual = Manual[0] + Manual[1] * HS_math_GPA + Manual[2] * HS_calc + Manual[3] * NTU_precalc\n",
    "pred_lm = lm[0] + lm[1] * HS_math_GPA + lm[2] * HS_calc + lm[3] * NTU_precalc\n",
    "\n",
    "pred_table = pd.DataFrame({\"Truth\": pred_truth, \"Manual\": pred_manual, \"lm\": pred_lm }, index = [\"prediction\"])\n",
    "pred_table"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cdc24ae0c741b64722937ee9139de5c3349dbeb6ed5467cc42a4fed5f500b0a1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
